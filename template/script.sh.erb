#!/usr/bin/env bash

# Benchmark info
echo "TIMING - Starting main script at: $(date)"

# Set working directory to home directory
cd "${HOME}"

# Purge the module environment to avoid conflicts
module purge

# Benchmark info
echo "TIMING - Starting llama.cpp at: $(date)"



llm_model=<%= context.llm_model -%>

echo $llm_model
echo $host

num_cpus=<%= context.ec_cpus -%>


oodllm_aux_version=20250617_2049b7a

oodllm_aux_dir=/cluster/opt/OOD/ood-llm-aux/${oodllm_aux_version}

echo "Starting proxy..."
oodproxy=${oodllm_aux_dir}/oodproxy/oodproxy
${oodproxy} --listen ${host}:${port} \
            --ipv4-range "193.156.40.0/22" \
            --ipv6-range "2001:700:5800::/41" \
            --model-script "
              ${oodllm_aux_dir}/runllm.sh
                ${llm_model}
                --threads ${num_cpus}
                --host /tmp/llamacpp.sock
              "

